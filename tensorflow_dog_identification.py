# -*- coding: utf-8 -*-
"""tensorflow_dog_identification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hNiYJjLNnX3mZZjB-mZmiIcpSxjGvTf3
"""

# from google.colab import drive
# drive.mount('/content/drive')

#!unzip "drive/MyDrive/dog-folder/dog-breed-identification.zip" -d "drive/MyDrive/dog-folder/"

import tensorflow as tf
import tensorflow_hub as hub
print("Tensorflow version: ", tf.__version__)
print("TF Hub version: ", hub.__version__)

#Check for GPU availability
print("GPU", "available(YESSS!!!)" if tf.config.list_physical_devices("GPU") else "not available")

#with open("drive/MyDrive/dogfolder/labels.csv") as f:
#  print(f.readlines())

import pandas as pd
labels = pd.read_csv("drive/MyDrive/dogfolder/labels.csv")
print(labels.describe())
print(labels.head())

labels.breed.unique()

len(labels)

labels.breed.value_counts().to_frame()



labels['breed'].value_counts().plot.bar(figsize=(20,10))

labels['breed'].value_counts().median()

from IPython.display import Image
Image('drive/MyDrive/dogfolder/train/0021f9ceb3235effd7fcde7f7538ed62.jpg')

labels.head(10)

filename = "drive/MyDrive/dogfolder/train/" + labels['id'] + ".jpg"
filename[:10]

filename = filename[1:]

filename

type(filename)

len(filename)

labels['id'][1:]

filename2 = 'drive/MyDrive/dogfolder/train/' + labels['id'][1:] + '.jpg'

filename2

#labels_id = labels["id"]+".jpg"

#labels[labels["id"]=="000bec180eb18c7604dcecc8fe0dba07.jpg"]

#labels.drop(["000bec180eb18c7604dcecc8fe0dba07.jpg"],axis=0)

#set(labels_id) - set(os.listdir("drive/MyDrive/dog-folder/train/"))

#set(os.listdir("drive/MyDrive/dog-folder/train/")) - set(labels_id)

#os.listdir("drive/MyDrive/dog-folder/train/")

os.listdir('drive/MyDrive/')

import os
len(os.listdir("drive/MyDrive/dogfolder/train/"))

import numpy as np
new_label = labels.iloc[1:,:]
labels_breed = new_label["breed"]
labels_breed = np.array(labels_breed)

# labels_breed = labels[1:]
# labels_breed = np.array(labels_breed)

len(labels_breed)

new_label = labels.iloc[1:,:]
new_label

labels_breed

len(labels_breed)

unique_breed = np.unique(labels_breed)

unique_breed

bool_label =[]
for label in labels_breed:
   bool_label.append([label  == unique_breed])

len(bool_label)







bool_label

boolean_labels = [labels_b == unique_breed for labels_b in labels_breed]
boolean_labels[:2]

len(boolean_labels[1])

X = filename
y = boolean_labels

NUM_IMAGES = 1000 #@param {type:"slider", min:1000, max:10000, step:1000}



from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],y[:NUM_IMAGES], test_size=0.2, random_state=42)

len(X_train), len(X_val), len(y_train), len(y_val)

"""Preprocessing Images (Converting images into tensors)"""



"""To preprocess
1. Take an image filepath as input
2. Use tensorflow to read the file and save it to a variable, image
3. Turn our image (a jpg) into tensors
4. Resize the image to be a shape of (192,192)
5. Return the modified image
"""

from matplotlib.pyplot import imread
image = imread(filename[31])
image.shape



image.max(), image.min()

tf.constant(image)[:2]



tf.constant(imread(X_train))

imag = tf.io.read_file(filename[21])
tf.image.decode_jpeg(imag, channels =3)

IMG_SIZE =192

def process_image(image_path, img_size=IMG_SIZE):

  #Read the image file
  image = tf.io.read_file(image_path)
  #Turn the jpeg into numerical Tensor with 3 colour channel(Red, Green, Blue[RGB])
  image = tf.image.decode_jpeg(image, channels=3)
  #Normalize the value from [0,255] to [0,1]
  image = tf.image.convert_image_dtype(image, tf.float32)
  #Resize the image in 224,224
  image = tf.image.resize(image, size=[img_size,img_size])

  return image

"""Turn our data into Batches"""

#We need our data in Tensor tuple form
def get_image_labels(image_path, label):
  image = process_image(image_path)
  return image,label

(tf.data.Dataset.from_tensor_slices(tf.constant(X_train))).map(process_image).batch(32)

BATCH_SIZE = 32

def create_data_batches(X, y=None, batch_size=BATCH_SIZE, valid_data=False, test_data=False):


  if test_data:
    print("Creating test data batches...")
    data = tf.data.Dataset.from_tensor_slices(tf.constant(X))
    data_batch = data.map(process_image).batch(BATCH_SIZE)
    return data_batch

  elif valid_data:
    print("Creating valid data batches")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),
                                              tf.constant(y)))
    data_batch = data.map(get_image_labels).batch(BATCH_SIZE)
    return data_batch

  else:
    print("Creating train data batches")
    data = tf.data.Dataset.from_tensor_slices((tf.constant(X),
                                              tf.constant(y)))
    data = data.shuffle(buffer_size=len(X))

    data = data.map(get_image_labels)

    data_batch = data.batch(BATCH_SIZE)
  return data_batch

#Create training and validation data batches
train_data = create_data_batches(X_train, y_train)
val_data = create_data_batches(X_val, y_val, valid_data=True)

val_data.element_spec

# how to check data of this --   tf.Size(val_data)

import matplotlib.pyplot as plt
def show_25_images(images,labels):
  plt.figure(figsize=(10,10))

  for i in range(25):
    ax = plt.subplot(5,5,i+1)
    plt.imshow(images[i])
    plt.title(unique_breed[labels[i].argmax()])
    plt.axis("off")

train_images, train_labels = next(train_data.as_numpy_iterator())
show_25_images(train_images,train_labels)

val_images, val_labels = next(val_data.as_numpy_iterator())
show_25_images(val_images,val_labels)

"""Importing model from tensorflow hub"""

#Setup input shape to the model
INPUT_SHAPE = [None,IMG_SIZE,IMG_SIZE,3]

#Setup output shape of our model
OUTPUT_SHAPE = len(unique_breed)

#Setup model URL from Tensorflow Hub
MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_035_192/classification/5"

def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
  print("Building model with:", MODEL_URL)

  model = tf.keras.Sequential([
     hub.KerasLayer(MODEL_URL),tf.keras.layers.Dense(units=OUTPUT_SHAPE, activation="softmax")]) #Layer 2 (output layer)

  # Compile the Mode
  model.compile(loss=tf.keras.losses.CategoricalCrossentropy(),optimizer=tf.keras.optimizers.Adam(),metrics=["accuracy"])

  #Build the model
  model.build(INPUT_SHAPE)

  return model

model = create_model()
model.summary()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard

import datetime

def create_tensorboard_callback():
  logdir = os.path.join("drive/MyDrive/dogfolder/logs/fit/",
                        datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))

  return tf.keras.callbacks.TensorBoard(logdir)

#Create earlystopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor="val_accuracy", patience=3)

NUM_EPOCHS = 100 #@param {type:"slider", min:10, max:100, step:10}

print("GPU", "Available, Yes!!!!" if tf.config.list_physical_devices("GPU") else "not available")

"""Create a function which trains a model.

*Create a model using `create_model()`
*Setup a tensorBoard callback using `create_tensorboard_callback()`
*Call the fit() function on our model passing it the trainig data, validation data, number of epochs to train for (`NUM_EPOCHS`) and callback which we like to use
*Return the model
"""

def train_model():
  model = create_model()

  tensorboard = create_tensorboard_callback()

  model.fit(x=train_data,
            epochs=NUM_EPOCHS,
            validation_data=val_data,
            validation_freq=1,
            callbacks=[tensorboard,early_stopping])
  return model

model = train_model()

NUM_EPOCHS

# %tensorboard --logdir drive/MyDrive/dogfolder/logs

"""Making and evaluting model predicition from model url which we have set above in our data"""

val_data

#Make prediciton on validation data
predictions = model.predict(val_data, verbose=1) # verbose is like when you are making prediction show me your progress
predictions

predictions.shape

np.sum(predictions[3])

#First Predictions
index=99
print(f"For val_data index {index}")
print(f"Max value (probability of prediction) : {np.max(predictions[index])}")
print(f"Sum : {np.sum(predictions[index])}")
print(f"Max index: {np.argmax(predictions[index])}")
print(f"Predicted label: {unique_breed[np.argmax(predictions[index])]}")

def get_pred_label(predictions_probability):

  return unique_breed[np.argmax(predictions_probability)]

pred_label = get_pred_label(predictions[89])
pred_label

#Create a function to unbatch a batch dataset

def unbatchify(data):
  images_ =[]
  labels_ = []

  #Loop through unbatched data
  for image,label in data.unbatch().as_numpy_iterator():

    images_.append(image)
    labels_.append(unique_breed[np.argmax(label)])  #
  return images_,labels_

val_images, val_labels = unbatchify(val_data)

print(val_images[22])

print(predictions[22])

# get_pred_label(val_images[39]) later check

get_pred_label(predictions[99])

"""Create function to compare for predictions and ground truth"""

def plot_pred(predictions_probability,labels, image, n=1):
  """
  View the prediction, ground truth and image for sample n
  """
  pred_prob, true_label, image = predictions_probability[n], labels[n], image[n]

  pred_label = get_pred_label(pred_prob)

  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  #Change the colour of the title depending on if the prediction is right or wrong
  if pred_label == true_label:
    color = "green"
  else:
    color = "red"

  # Change plot title to be predicted, probability of prediction and truth label
  plt.title("{} {:.2f}% {}" .format(pred_label,
                                     np.max(pred_prob)*100,
                                      true_label),color=color)

plot_pred(predictions_probability=predictions,
          labels=val_labels,
          image=val_images, n=54)

"""Making Prediction on top 10 data labels"""

def plot_pred_conf(predictions_probability, labels,n=1):
  """
  Plus the top 10 highest prediction confidences along with the truth label for sample n.
  """
  pred_prob, true_label = predictions_probability[n], labels[n]

  # Get the predicted label
  pred_label = get_pred_label(pred_prob)

  #Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # Find the top 10 prediciton confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]
  #Find the top 10 prediction labels
  top_10_pred_labels = unique_breed[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)),
                     top_10_pred_values*100,
                     color='grey')
  plt.xticks(np.arange(len(top_10_pred_labels)),
              labels=top_10_pred_labels,
              rotation='vertical')

  #Change color of true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
  else:
    pass

plot_pred_conf(predictions_probability=predictions, labels=val_labels, n=65)

# Let's check out a few predictions and their different values

i_multiplier = 20
num_rows = 5
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(10*num_cols, 5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(predictions_probability=predictions,
            labels=val_labels,
            image=val_images,
            n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(predictions_probability=predictions,
                 labels=val_labels,
                 n=i+i_multiplier)
plt.tight_layout(h_pad=1.0)
plt.show()

"""Saving a Model"""

def save_model(model, suffix=None):
  """
  Saves a given model in models directory and appends a suffix(string)
  """
  #Create models directory pathname with current time
  modeldir = os.path.join("drive/MyDrive/dogfolder/model",
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%s"))
  model_path = modeldir + "-" + suffix + ".h5"  #save format of model
  print(f"Saving model to: {model_path}...")
  model.save(model_path)
  return model_path

# Create a function to load a trained model
def load_model(model_path):
  """
  Loads a saved model from a specified path.
  """

  print(f"Loading saved model from: {model_path}")
  model = tf.keras.models.load_model(model_path,
                                     custom_objects={"KerasLayer":hub.KerasLayer})
  return model

# Save model trained on 1000 images
 save_model(model, suffix="1000-images-mobilenetvet2-Adam")

# Load a trained model
loaded_1000_image_model = load_model("drive/MyDrive/dogfolder/model/20231104-12181699100315-1000-images-mobilenetvet2-Adam.h5")

# Evaluate the pre-saved model
model.evaluate(val_data)

#Evaluate the loaded model
loaded_1000_image_model.evaluate(val_data)

"""Training a big dog model (on the full data)"""

len(X), len(y)

# Create a data batch with the full data
full_data = create_data_batches(X,y)

full_data

full_model = create_model()

full_model_tensorboard = create_tensorboard_callback()

full_model_early_stopping = tf.keras.callbacks.EarlyStopping(monitor="accuracy",patience=3)

# Fit the full model
full_model.fit(x=full_data, epochs=NUM_EPOCHS,callbacks=[full_model_tensorboard, full_model_early_stopping])

save_model(full_model, suffix='full-image-mobilenetv2-Adam')

loading_full_model = load_model('drive/MyDrive/dogfolder/model/20231104-13491699105751-full-image-mobilenetv2-Adam.h5')

# Making prediction on test dataset
test_path = "drive/MyDrive/dogfolder/test/"
test_filename = [test_path + fname for fname in os.listdir(test_path)]
test_filename[:10]

len(test_filename)

test_data = create_data_batches(test_filename,test_data=True)

test_data



test_predictions = loading_full_model.predict(test_data, verbose=1)

# Save prediction
np.savetxt("drive/MyDrive/dogfolder/preds_array.csv", test_predictions, delimiter=",")

# Load Predictions
test_predictions = np.loadtxt("drive/MyDrive/dogfolder/preds_array.csv", delimiter=",")

test_predictions[:10]

test_predictions.shape

# Creating dataframe for submitting
preds_df = pd.DataFrame(columns = ["id"] + list(unique_breed))
preds_df

type(list(unique_breed))

test_ids = [os.path.splitext(path)[0] for path in os.listdir(test_path)]
preds_df["id"] = test_ids

preds_df.head()

preds_df[list(unique_breed)] = test_predictions

preds_df.head()

# Save submission prediction for test database in csv format for kaggle dog breed identification
preds_df.to_csv("drive/MyDrive/dogfolder/full_model_predictions_submission_mobilnetV2.csv", index=False)

# Making prediction from our model which is train from 10000 images to predict(find) dog breed from dog photos
custom_path = "drive/MyDrive/dogfolder/dogphotos/"
custom_image_paths = [custom_path + fname for fname in os.listdir(custom_path)]

type(custom_path)

type( os.listdir(custom_path))

custom_image_paths

#custom_image_paths = custom_image_paths[:2]

os.listdir(custom_path)











# Turn custom images into batch datasets
custom_data = create_data_batches(custom_image_paths, test_data = True)

custom_data

custom_preds = loading_full_model.predict(custom_data)

custom_preds.shape

# Get custom image prediction labels
custom_preds_labels = [get_pred_label(custom_preds[i]) for i in range(len(custom_preds))]
custom_preds_labels

# Get custom images
custom_images = []
# Loop throug unbatched data
for image in custom_data.unbatch().as_numpy_iterator():
  custom_images.append(image)



# check custom image predictions
plt.figure(figsize=(10, 10))
for i, image in enumerate(custom_images):
  plt.subplot(1, 6, i+1)
  plt.xticks([])
  plt.yticks([])
  plt.title(custom_preds_labels[i])
  plt.tight_layout(h_pad=1.0)
  plt.imshow(image)

















